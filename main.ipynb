{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "b926ad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zavier Morales\n",
    "# Edouard Mason\n",
    "from pathlib import Path\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import randint\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from scipy.stats import loguniform\n",
    "import optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING) \n",
    "\n",
    "# Define global variables & import data\n",
    "\n",
    "RANDOM_STATE_ID = 100577770\n",
    "\n",
    "TRAINING_DATA_PATH = Path('dataset', 'bank_06.pkl')\n",
    "COMPETITION_DATA_PATH = Path('dataset', 'bank_competition.pkl')\n",
    "\n",
    "training_times = {}\n",
    "tuning_times = {}\n",
    "prediction_times = {}\n",
    "\n",
    "raw_data = pd.read_pickle(TRAINING_DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fcefc7",
   "metadata": {},
   "source": [
    "<div style= \"font-family: 'Georgia', serif;\" >\n",
    "\n",
    "## **EDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e045424a",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 1000px; font-family: 'Georgia', serif; font-size: 14px; line-height: 1.6;\">\n",
    "\n",
    "Our goal in EDA is to ensure the following about our dataset:\n",
    "\n",
    "<br>\n",
    "\n",
    "- All variables are in numerical format.\n",
    "- There are no missing values in our dataset.\n",
    "- Our dataset is well balanced (this is a classification problem since we are predicting whether a customer will subscribe or not)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6ca3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset:\n",
      " (11000, 17)\n",
      "\n",
      "First few rows:\n",
      "    age         job  marital  education default  balance housing loan  contact  \\\n",
      "0   59      admin.  married  secondary      no     2343     yes   no  unknown   \n",
      "1   56      admin.  married  secondary      no       45      no   no  unknown   \n",
      "2   41  technician  married  secondary      no     1270     yes   no  unknown   \n",
      "3   55    services  married  secondary      no     2476     yes   no  unknown   \n",
      "4   54      admin.  married   tertiary      no      184      no   no  unknown   \n",
      "\n",
      "   day month  duration  campaign  pdays  previous poutcome deposit  \n",
      "0    5   may      1042         1     -1         0  unknown     yes  \n",
      "1    5   may      1467         1     -1         0  unknown     yes  \n",
      "2    5   may      1389         1     -1         0  unknown     yes  \n",
      "3    5   may       579         1     -1         0  unknown     yes  \n",
      "4    5   may       673         2     -1         0  unknown     yes  \n",
      "\n",
      "List of columns:\n",
      " Index(['age', 'job', 'marital', 'education', 'default', 'balance', 'housing',\n",
      "       'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays',\n",
      "       'previous', 'poutcome', 'deposit'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "\n",
      "Target variable: 0    yes\n",
      "1    yes\n",
      "2    yes\n",
      "3    yes\n",
      "4    yes\n",
      "Name: deposit, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# quick look at our dataset to help us determine what to do first (probably convert categorical to numerical)\n",
    "print(\"Shape of the dataset:\\n\", raw_data.shape)\n",
    "print(\"\\nFirst few rows:\\n\",raw_data.head())\n",
    "print(\"\\nList of columns:\\n\",raw_data.columns)\n",
    "print(\"\\n\\n\\nTarget variable:\\n\",raw_data['deposit'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d566057d",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 1000px; font-family: 'Georgia', serif; font-size: 14px; line-height: 1.6;\">\n",
    "\n",
    "Here we are finding the cardinality & data types of variables in order to determine which variables are object-like vs already numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1188cfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age            76\n",
      "job            12\n",
      "marital         3\n",
      "education       4\n",
      "default         2\n",
      "balance      3783\n",
      "housing         2\n",
      "loan            2\n",
      "contact         3\n",
      "day            31\n",
      "month          12\n",
      "duration     1423\n",
      "campaign       36\n",
      "pdays         472\n",
      "previous       34\n",
      "poutcome        4\n",
      "deposit         2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Cardinality of variables\n",
    "unique_counts = raw_data.nunique()\n",
    "print(unique_counts)\n",
    "print(raw_data.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabfc7d3",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 1000px; font-family: 'Georgia', serif; font-size: 14px; line-height: 1.6;\">\n",
    "\n",
    "We use one-hot encoding to create new columns for the categorical variables we just identified. We also check if there are any missing values per column, and luckily we find that there are none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012a3f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns in the DataFrame:\n",
      "\n",
      " Index(['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous',\n",
      "       'deposit', 'job_admin.', 'job_blue-collar', 'job_entrepreneur',\n",
      "       'job_housemaid', 'job_management', 'job_retired', 'job_self-employed',\n",
      "       'job_services', 'job_student', 'job_technician', 'job_unemployed',\n",
      "       'job_unknown', 'marital_divorced', 'marital_married', 'marital_single',\n",
      "       'education_primary', 'education_secondary', 'education_tertiary',\n",
      "       'education_unknown', 'default_no', 'default_yes', 'housing_no',\n",
      "       'housing_yes', 'loan_no', 'loan_yes', 'contact_cellular',\n",
      "       'contact_telephone', 'contact_unknown', 'month_apr', 'month_aug',\n",
      "       'month_dec', 'month_feb', 'month_jan', 'month_jul', 'month_jun',\n",
      "       'month_mar', 'month_may', 'month_nov', 'month_oct', 'month_sep',\n",
      "       'poutcome_failure', 'poutcome_other', 'poutcome_success',\n",
      "       'poutcome_unknown'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "First few rows of the DataFrame:\n",
      "\n",
      "    age  balance  day  duration  campaign  pdays  previous deposit  job_admin.  \\\n",
      "0   59     2343    5      1042         1     -1         0     yes           1   \n",
      "1   56       45    5      1467         1     -1         0     yes           1   \n",
      "2   41     1270    5      1389         1     -1         0     yes           0   \n",
      "3   55     2476    5       579         1     -1         0     yes           0   \n",
      "4   54      184    5       673         2     -1         0     yes           1   \n",
      "\n",
      "   job_blue-collar  ...  month_jun  month_mar  month_may  month_nov  \\\n",
      "0                0  ...          0          0          1          0   \n",
      "1                0  ...          0          0          1          0   \n",
      "2                0  ...          0          0          1          0   \n",
      "3                0  ...          0          0          1          0   \n",
      "4                0  ...          0          0          1          0   \n",
      "\n",
      "   month_oct  month_sep  poutcome_failure  poutcome_other  poutcome_success  \\\n",
      "0          0          0                 0               0                 0   \n",
      "1          0          0                 0               0                 0   \n",
      "2          0          0                 0               0                 0   \n",
      "3          0          0                 0               0                 0   \n",
      "4          0          0                 0               0                 0   \n",
      "\n",
      "   poutcome_unknown  \n",
      "0                 1  \n",
      "1                 1  \n",
      "2                 1  \n",
      "3                 1  \n",
      "4                 1  \n",
      "\n",
      "[5 rows x 52 columns]\n",
      "\n",
      "Missing values per column:\n",
      "\n",
      " age                    0\n",
      "balance                0\n",
      "day                    0\n",
      "duration               0\n",
      "campaign               0\n",
      "pdays                  0\n",
      "previous               0\n",
      "deposit                0\n",
      "job_admin.             0\n",
      "job_blue-collar        0\n",
      "job_entrepreneur       0\n",
      "job_housemaid          0\n",
      "job_management         0\n",
      "job_retired            0\n",
      "job_self-employed      0\n",
      "job_services           0\n",
      "job_student            0\n",
      "job_technician         0\n",
      "job_unemployed         0\n",
      "job_unknown            0\n",
      "marital_divorced       0\n",
      "marital_married        0\n",
      "marital_single         0\n",
      "education_primary      0\n",
      "education_secondary    0\n",
      "education_tertiary     0\n",
      "education_unknown      0\n",
      "default_no             0\n",
      "default_yes            0\n",
      "housing_no             0\n",
      "housing_yes            0\n",
      "loan_no                0\n",
      "loan_yes               0\n",
      "contact_cellular       0\n",
      "contact_telephone      0\n",
      "contact_unknown        0\n",
      "month_apr              0\n",
      "month_aug              0\n",
      "month_dec              0\n",
      "month_feb              0\n",
      "month_jan              0\n",
      "month_jul              0\n",
      "month_jun              0\n",
      "month_mar              0\n",
      "month_may              0\n",
      "month_nov              0\n",
      "month_oct              0\n",
      "month_sep              0\n",
      "poutcome_failure       0\n",
      "poutcome_other         0\n",
      "poutcome_success       0\n",
      "poutcome_unknown       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Encoding object-type variables (except for deposit, which is our target variable)\n",
    "df_encoded = pd.get_dummies(raw_data, columns=['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome'])\n",
    "print(df_encoded)\n",
    "\n",
    "#Converting all boolean columns to integers (1 for True, 0 for False)\n",
    "df_encoded[df_encoded.select_dtypes(include=['bool']).columns] = df_encoded.select_dtypes(include=['bool']).astype(int)\n",
    "print(\"\\nColumns in the DataFrame:\\n\\n\",df_encoded.columns)\n",
    "print(\"\\n\\nFirst few rows of the DataFrame:\\n\\n\",df_encoded.head())\n",
    "\n",
    "#Checking the number of missing values per column\n",
    "missing_values = df_encoded.isnull().sum()\n",
    "print(\"\\nMissing values per column:\\n\\n\", missing_values)   #We get 0 for each column so we have no missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fa2d9b",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 1000px; font-family: 'Georgia', serif; font-size: 14px; line-height: 1.6;\">The issue with the pdays variable is that the variable encodes multiple meanings: the duration between contacts and the status of the contact. Instead, we must figure out a way to modify pdays or create new columns to preserve the different meanings while maintaining that 1) the variables are numerical and 2) only encodes one meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "5e326819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8203\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where pdays == -1\n",
    "count_minus1 = (raw_data['pdays'] == -1).sum()\n",
    "\n",
    "print(count_minus1)\n",
    "\n",
    "# We see that there are 8203 where pdays = -1, that is where we have no info or contact. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914597c5",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 1000px; font-family: 'Georgia', serif; font-size: 14px; line-height: 1.6;\">\n",
    "Our method is to create a new dummy variable prev_contacted where 1 means the value for pdays was not -1, and 0 where it was -1. This dummy variable only encodes the status of the contact with the customer, and satisfies our two criteria above (numerical and encodes one meaning).\n",
    "<br>\n",
    "\n",
    "We then replace pdays with pdays_duration where all values that were originally -1 are instead the median of all positive pdays values. This new variable is both numerical and only encodes one meaning (contact duration) which satisfies our two criteria above. Given that -1 means either we don't know the outcome or we know the customer never resonded, then using the median duration period assumes that these customers are likely similar enough to the average customer who did respond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bb73b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182.0\n"
     ]
    }
   ],
   "source": [
    "# We create a new binary column with value = 1 if previously contacted and 0 if pdays = -1 (no previous contact)\n",
    "df_encoded[\"prev_contacted\"] = (df_encoded[\"pdays\"] != -1).astype(int)\n",
    "# We only calculate the median on values greater than -1\n",
    "known_pdays = df_encoded.loc[df_encoded['pdays'] != -1, \"pdays\"]\n",
    "median_pdays = known_pdays.median()\n",
    "print(median_pdays)\n",
    "\n",
    "# We replace all -1 values with the median of the know_pdays\n",
    "df_encoded['pdays_duration'] = df_encoded['pdays'].replace(-1, median_pdays)\n",
    "print(df_encoded['pdays_duration'].tail())\n",
    "print(df_encoded.tail())\n",
    "\n",
    "# We drop the original pdays variable\n",
    "df_encoded.drop('pdays', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cc10dd",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 1000px; font-family: 'Georgia', serif; font-size: 14px; line-height: 1.6;\">\n",
    "After we have all of our training variables cleaned and in the format we want them in, we finally convert our target variable deposit to a binary integer value. Then, to prepare our data for training and test validation using the holdout method, we divide our data into \"attributes\" and \"class\" sets, and then divide those sets into \"test\" and \"training\".\n",
    "<br>\n",
    "\n",
    "In order to scale our numerical/continuous columns (excluding the categorical columns created with one-hot encoding), we use a transformer to discriminate and scale the numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "c8104d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We convert yes to 1 and no to 0 for the target 'deposit'\n",
    "df_encoded[\"deposit\"] = df_encoded[\"deposit\"].map({\"yes\": 1, \"no\": 0}) #Used AI to convert yes to 1 and no to 0 in the target column 'deposit'\n",
    "\n",
    "y = df_encoded[\"deposit\"]\n",
    "X = df_encoded.drop(\"deposit\", axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE_ID)\n",
    "\n",
    "num_cols = ['age', 'balance', 'day', 'duration', 'campaign', 'previous', 'pdays_duration']\n",
    "cat_cols = [col for col in df_encoded.columns if col not in num_cols and col != 'deposit']  # Used AI to create the loop selecting all columns that are not in num_cols and are not 'deposit'\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_cols), \n",
    "        ('cat', 'passthrough', cat_cols) \n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0224e06d",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 1000px; font-family: 'Georgia', serif; font-size: 14px; line-height: 1.6;\">\n",
    "We also want to consider if our data is imbalanced or not. From the graph below, we can see that the distribution of the classes for our target variable are relatively evenly distributed. When we calculate the proportion of each class, we can see that the 0 class only makes up about 53% of the total distribution, meaning that we should aim for our model to have an accuracy greater than 52%. This is to say that our dataset is not imbalanced as long as we don't have a model that performs incredibly poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "7300ef4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(y).value_counts().plot(kind='bar')\n",
    "plt.title('Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b1028f",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 1000px; font-family: 'Georgia', serif; font-size: 14px; line-height: 1.6;\">\n",
    "\n",
    "## <u>**Class Distribution**</u>\n",
    "\n",
    "Our test partition is 20% of the overall dataset. A rule of thumb when deciding the test size for the holdout method is that as the size of the dataset increases, the size of the test partition is able to become smaller while still maintaining the same margin of error for measuring the estimated accuracy. With the below calculated margins of error that are all small (none above 0.017), we obtain precise and reliable accuracy measures and taking into consideration our large dataset size (11,000 observations), we have sufficient arguments validating our choice of an 80/20 split. We will thus pursue the analysis with this split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f4ee72",
   "metadata": {},
   "source": [
    "<div style= \"font-family: 'Georgia', serif;\" >\n",
    "\n",
    "## <u> **Base Models**</u>\n",
    "\n",
    "### **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "75348c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8181818181818182\n",
      "0.0718069076538086\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(random_state=RANDOM_STATE_ID)\n",
    "\n",
    "start_time = time.time()\n",
    "logreg.fit(X_train_processed, y_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "y_pred_log = logreg.predict(X_test_processed)\n",
    "\n",
    "accuracy_log = accuracy_score(y_test, y_pred_log)\n",
    "\n",
    "print(accuracy_log)\n",
    "print(training_time)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "51bda515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8020646663080251 0.8342989700556114\n",
      "0.016117151873793183\n"
     ]
    }
   ],
   "source": [
    "#Confidence Interval\n",
    "\n",
    "n = len(y_test)\n",
    "z = 1.96\n",
    "\n",
    "se_log = np.sqrt((accuracy_log*(1-accuracy_log))/n) \n",
    "\n",
    "ci_upper = accuracy_log + se_log*z\n",
    "ci_lower = accuracy_log - se_log*z\n",
    "\n",
    "print(ci_lower, ci_upper)\n",
    "print(se_log*z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca14a571",
   "metadata": {},
   "source": [
    "<div style= \"font-family: 'Georgia', serif;\" >\n",
    "\n",
    "\n",
    "### **KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "d9ce2aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7995454545454546\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "start_time = time.time()\n",
    "knn.fit(X_train_processed, y_train)\n",
    "training_times[\"knn\"] = time.time() - start_time\n",
    "\n",
    "y_pred_knn = knn.predict(X_test_processed)\n",
    "\n",
    "accuracy_knn = accuracy_score(y_test, y_pred_knn)\n",
    "\n",
    "print(accuracy_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "2c4d0800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7828162895473836 0.8162746195435255\n",
      "0.016729164998070947\n"
     ]
    }
   ],
   "source": [
    "#Confidence Interval\n",
    "\n",
    "n = len(y_test)\n",
    "z = 1.96\n",
    "\n",
    "se_knn = np.sqrt((accuracy_knn*(1-accuracy_knn))/n) \n",
    "\n",
    "ci_upper = accuracy_knn + se_knn*z\n",
    "ci_lower = accuracy_knn - se_knn*z\n",
    "\n",
    "print(ci_lower, ci_upper)\n",
    "print(se_knn*z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437ac996",
   "metadata": {},
   "source": [
    "<div style= \"font-family: 'Georgia', serif;\" >\n",
    "\n",
    "\n",
    "### **Tree** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "8e83636c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7868181818181819\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "start_time = time.time()\n",
    "tree = tree.fit(X_train_processed, y_train)\n",
    "training_times[\"tree\"] = time.time() - start_time\n",
    "\n",
    "y_pred_tree = tree.predict(X_test_processed)\n",
    "\n",
    "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
    "\n",
    "print(accuracy_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "a244f41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7697039659575906 0.8039323976787731\n",
      "0.01711421586059131\n"
     ]
    }
   ],
   "source": [
    "#Confidence Interval\n",
    "\n",
    "n = len(y_test)\n",
    "z = 1.96\n",
    "\n",
    "se_tree = np.sqrt((accuracy_tree*(1-accuracy_tree))/n) \n",
    "\n",
    "ci_upper = accuracy_tree + se_tree*z\n",
    "ci_lower = accuracy_tree - se_tree*z\n",
    "\n",
    "print(ci_lower, ci_upper)\n",
    "print(se_tree*z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d16d379",
   "metadata": {},
   "source": [
    "<div style= \"font-family: 'Georgia', serif;\" >\n",
    "\n",
    "\n",
    "### **Dummy Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "c713666d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.519090909090909\n"
     ]
    }
   ],
   "source": [
    "dummy = DummyClassifier(strategy='most_frequent', random_state=42)\n",
    "dummy.fit(X_train, y_train)\n",
    "y_pred_dummy = dummy.predict(X_test)\n",
    "accuracy_dummy = accuracy_score(y_test, y_pred_dummy)\n",
    "\n",
    "print(accuracy_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21119137",
   "metadata": {},
   "source": [
    "<div style= \"font-family: 'Georgia', serif;\" >\n",
    "\n",
    "## <u>**Model Comparison**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a535f785",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 1000px; font-family: 'Georgia', serif; font-size: 14px; line-height: 1.6;\">\n",
    "\n",
    "## **Results**\n",
    "\n",
    "| Model | Accuracy - no hyperparameter tuning |95 % Confidence Interval| Training time (s) |\n",
    "|-------|--------|---------|----------|\n",
    "|Logistic Regression|0.8182|[0.802, 0.834]|0.0529|\n",
    "|KNN|0.7995|[0.783, 0.816]|2.1|0.0039|\n",
    "|Decision Tree|0.785|[0.7604, 0.7951]|6.6|0.0802|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Dummy Classifier accuracy : 0.519\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "The model with the highest accuracy is the Logistic Regression (81.82%). It also has the lowest training time with 0.0529 seconds. On the other hand, the dummy classifier only achieved 51.9 % accuracy. The Logistic Regression improves prediction accuracy by almost 30 percentage points, meaning that it is much more performant than the dummy method. This reinforces the fact that our data is not imbalanced since the na√Øve baseline only correctly predicts the target variable a bit over 50 % of the time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0942093",
   "metadata": {},
   "source": [
    "<div style= \"font-family: 'Georgia', serif;\" >\n",
    "\n",
    "\n",
    "\n",
    "## <u>**Hyper-parameter Tuning**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60efa349",
   "metadata": {},
   "source": [
    "<div style= \"font-family: 'Georgia', serif;\" >\n",
    "\n",
    "\n",
    "\n",
    "### **Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e6133f",
   "metadata": {},
   "source": [
    "<div style= \"font-family: 'Georgia', serif;\" >\n",
    "\n",
    "\n",
    "\n",
    "##### **Discrete Parametrical Space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "63ec4dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 100, 'max_iter': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Best CV score: 0.8322\n",
      "Time: 6.97s\n",
      "Test accuracy: 0.8159\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid_discrete = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['lbfgs', 'liblinear'],\n",
    "    'max_iter': [100, 500, 1000]\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "grid_discrete = GridSearchCV(\n",
    "    LogisticRegression(random_state=RANDOM_STATE_ID),\n",
    "    param_grid_discrete,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_discrete.fit(X_train_processed, y_train)\n",
    "grid_time_discrete = time.time() - start\n",
    "\n",
    "print(f\"Best params: {grid_discrete.best_params_}\")\n",
    "print(f\"Best CV score: {grid_discrete.best_score_:.4f}\")\n",
    "print(f\"Time: {grid_time_discrete:.2f}s\")\n",
    "\n",
    "y_pred_grid_discrete = grid_discrete.predict(X_test_processed)\n",
    "acc_grid_discrete = accuracy_score(y_test, y_pred_grid_discrete)\n",
    "print(f\"Test accuracy: {acc_grid_discrete:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "a8dd1750",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'solver': 'liblinear', 'penalty': 'l2', 'max_iter': 1000, 'C': 100}\n",
      "Best CV score: 0.8322\n",
      "Time: 2.59s\n",
      "Test accuracy: 0.81590909\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#RandomizedSearchCV with discrete values\n",
    "start = time.time()\n",
    "random_discrete = RandomizedSearchCV(\n",
    "    LogisticRegression(random_state=RANDOM_STATE_ID),\n",
    "    param_grid_discrete,  \n",
    "    n_iter=30,  \n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    random_state=RANDOM_STATE_ID,\n",
    "    n_jobs=-1\n",
    ")\n",
    "random_discrete.fit(X_train_processed, y_train)\n",
    "random_time_discrete = time.time() - start\n",
    "\n",
    "print(f\"Best params: {random_discrete.best_params_}\")\n",
    "print(f\"Best CV score: {random_discrete.best_score_:.4f}\")\n",
    "print(f\"Time: {random_time_discrete:.2f}s\")\n",
    "\n",
    "y_pred_random_discrete = random_discrete.predict(X_test_processed)\n",
    "acc_random_discrete = accuracy_score(y_test, y_pred_random_discrete)\n",
    "print(f\"Test accuracy: {acc_random_discrete:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e6a896",
   "metadata": {},
   "source": [
    "<div style= \"font-family: 'Georgia', serif;\" >\n",
    "\n",
    "\n",
    "\n",
    "##### **Continuous Parametrical Space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "8b9a6e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': np.float64(50.46250566733664), 'max_iter': 1503, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Best CV score: 0.8323\n",
      "Time: 3.47s\n",
      "Test accuracy: 0.81590909\n"
     ]
    }
   ],
   "source": [
    "param_dist_continuous = {\n",
    "    'C': loguniform(1e-4, 1e2),\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['lbfgs', 'liblinear'],\n",
    "    'max_iter': randint(100, 2000)  \n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "random_continuous = RandomizedSearchCV(\n",
    "    LogisticRegression(random_state=RANDOM_STATE_ID),\n",
    "    param_dist_continuous,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    random_state=RANDOM_STATE_ID,\n",
    "    n_jobs=-1\n",
    ")\n",
    "random_continuous.fit(X_train_processed, y_train)\n",
    "random_time_continuous = time.time() - start\n",
    "\n",
    "print(f\"Best params: {random_continuous.best_params_}\")\n",
    "print(f\"Best CV score: {random_continuous.best_score_:.4f}\")\n",
    "print(f\"Time: {random_time_continuous:.2f}s\")\n",
    "\n",
    "y_pred_random_continuous = random_continuous.predict(X_test_processed)\n",
    "acc_random_continuous = accuracy_score(y_test, y_pred_random_continuous)\n",
    "print(f\"Test accuracy: {acc_random_continuous:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "0adb8c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (4.6.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from optuna) (1.17.2)\n",
      "Requirement already satisfied: colorlog in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from optuna) (6.10.1)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from optuna) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/edouardmason/Library/Python/3.14/lib/python/site-packages (from optuna) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from optuna) (2.0.44)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from optuna) (6.0.3)\n",
      "Requirement already satisfied: Mako in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "fe7b0d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Library/Frameworks/Python.framework/Versions/3.14/lib/python3.14/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 100 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=100).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 1.494014024734497\n",
      "Best: {'C': 0.0495828472449947, 'solver': 'lbfgs'}\n",
      "Accuracy score:, 0.820\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    C = trial.suggest_float('C', 0.0001, 100, log=True)\n",
    "    solver = trial.suggest_categorical('solver', ['lbfgs', 'liblinear'])\n",
    "    \n",
    "    model = LogisticRegression(C=C, solver=solver, random_state=RANDOM_STATE_ID)\n",
    "    model.fit(X_train_processed, y_train)\n",
    "    \n",
    "    return accuracy_score(y_test, model.predict(X_test_processed))\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "optuna_logistic_time = time.time() - start\n",
    "\n",
    "print(\"Time\", optuna_logistic_time)\n",
    "print(\"Best:\", study.best_params)\n",
    "print(f\"Accuracy score:, {study.best_value:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "218cb422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016069973732012077\n",
      "0.8034754808134426 0.8356154282774666\n"
     ]
    }
   ],
   "source": [
    "#Confidence Interval\n",
    "\n",
    "n = len(y_test)\n",
    "z = 1.96\n",
    "\n",
    "se_log_tuned = np.sqrt((study.best_value*(1-study.best_value))/n) \n",
    "\n",
    "ci_upper = study.best_value + se_log_tuned*z\n",
    "ci_lower = study.best_value - se_log_tuned*z\n",
    "\n",
    "print(se_log_tuned*z)\n",
    "print(ci_lower, ci_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c5f712",
   "metadata": {},
   "source": [
    "<div style= \"font-family: 'Georgia', serif;\" >\n",
    "\n",
    "\n",
    "\n",
    "### **KNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5b4b9c",
   "metadata": {},
   "source": [
    "<div style= \"font-family: 'Georgia', serif;\" >\n",
    "\n",
    "\n",
    "\n",
    "##### **Discrete Parametrical Space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "6e042932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'metric': 'euclidean', 'n_neighbors': 20, 'weights': 'distance'}\n",
      "Best CV score: 0.8108\n",
      "Time: 10.12s\n",
      "Test accuracy: 0.8191\n"
     ]
    }
   ],
   "source": [
    "param_grid_knn_discrete = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11, 15, 20],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "grid_knn_discrete = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    param_grid_knn_discrete,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    ")\n",
    "grid_knn_discrete.fit(X_train_processed, y_train)\n",
    "grid_knn_time_discrete = time.time() - start\n",
    "\n",
    "print(f\"Best params: {grid_knn_discrete.best_params_}\")\n",
    "print(f\"Best CV score: {grid_knn_discrete.best_score_:.4f}\")\n",
    "print(f\"Time: {grid_knn_time_discrete:.2f}s\")\n",
    "\n",
    "y_pred = grid_knn_discrete.predict(X_test_processed)\n",
    "accuracy_knn_discrete = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test accuracy: {accuracy_knn_discrete:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "e905c5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'weights': 'distance', 'n_neighbors': 11, 'metric': 'manhattan'}\n",
      "Best CV score: 0.7516\n",
      "Time: 20.33s\n",
      "Test accuracy: 0.8191\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "random_knn_discrete = RandomizedSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    param_grid_knn_discrete,\n",
    "    n_iter=25,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    random_state=RANDOM_STATE_ID,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "random_knn_discrete.fit(X_train, y_train)\n",
    "random_knn_time_discrete = time.time() - start\n",
    "\n",
    "print(f\"Best params: {random_knn_discrete.best_params_}\")\n",
    "print(f\"Best CV score: {random_knn_discrete.best_score_:.4f}\")\n",
    "print(f\"Time: {random_knn_time_discrete:.2f}s\")\n",
    "\n",
    "y_pred_rs = grid_knn_discrete.predict(X_test_processed)\n",
    "accuracy_knn_discrete_rs = accuracy_score(y_test, y_pred_rs)\n",
    "print(f\"Test accuracy: {accuracy_knn_discrete_rs:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9164723",
   "metadata": {},
   "source": [
    "<div style= \"font-family: 'Georgia', serif;\" >\n",
    "\n",
    "\n",
    "\n",
    "##### **Continuous Parametrical Space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "dc1c9cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'weights': 'distance', 'n_neighbors': 29, 'metric': 'minkowski'}\n",
      "Best score: 0.8113636363636363\n",
      "Test accuracy: 0.8140909090909091\n",
      "Time: 4.58s\n"
     ]
    }
   ],
   "source": [
    "param_dist_knn_continuous = {\n",
    "    'n_neighbors': range(1, 31),\n",
    "    'weights': ['uniform', 'distance'], \n",
    "    'metric': ['euclidean','minkowski','manhattan']  \n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "random_search_knn_continuous = RandomizedSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    param_dist_knn_continuous,\n",
    "    n_iter=20, \n",
    "    cv=5,\n",
    "    random_state=RANDOM_STATE_ID,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search_knn_continuous.fit(X_train_processed, y_train)\n",
    "random_knn_time_continuous = time.time() - start\n",
    "\n",
    "print(\"Best parameters:\", random_search_knn_continuous.best_params_)\n",
    "print(\"Best score:\", random_search_knn_continuous.best_score_)\n",
    "\n",
    "rs_tuned_knn = random_search_knn_continuous.predict(X_test_processed)\n",
    "accuracy_tuned_knn = accuracy_score(rs_tuned_knn, y_test)\n",
    "\n",
    "print(\"Test accuracy:\", accuracy_tuned_knn)\n",
    "print(f\"Time: {random_knn_time_continuous:.2f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "a9a77432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2.7407758235931396\n",
      "Best: {'n_neighbors': 9, 'weights': 'distance'}\n",
      "Score: 0.8218181818181818\n"
     ]
    }
   ],
   "source": [
    "def objective_knn(trial):\n",
    "    n_neighbors = trial.suggest_int('n_neighbors', 1, 50)\n",
    "    weights = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
    "    \n",
    "    model = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights)\n",
    "    model.fit(X_train_processed, y_train)\n",
    "    \n",
    "    return accuracy_score(y_test, model.predict(X_test_processed))\n",
    "\n",
    "start = time.time()\n",
    "study_knn = optuna.create_study(direction='maximize')\n",
    "study_knn.optimize(objective_knn, n_trials=50)\n",
    "\n",
    "optuna_knn_time = time.time() - start\n",
    "\n",
    "print(\"Time:\", optuna_knn_time)\n",
    "print(\"Best:\", study_knn.best_params)\n",
    "print(\"Score:\", study_knn.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "7bb28e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8058275988644368 0.8378087647719268\n"
     ]
    }
   ],
   "source": [
    "#Confidence Interval\n",
    "\n",
    "n = len(y_test)\n",
    "z = 1.96\n",
    "\n",
    "se_knn_tuned = np.sqrt((study_knn.best_value*(1-study_knn.best_value))/n) \n",
    "\n",
    "ci_upper = study_knn.best_value + se_knn_tuned*z\n",
    "ci_lower = study_knn.best_value - se_knn_tuned*z\n",
    "\n",
    "print(ci_lower, ci_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4bf047",
   "metadata": {},
   "source": [
    "<div style= \"font-family: 'Georgia', serif;\" >\n",
    "\n",
    "\n",
    "### **Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "39f993f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters (Random Search): {'criterion': 'gini', 'max_depth': 13, 'min_samples_leaf': 18, 'min_samples_split': 3}\n",
      "Best Score (Random Search): 0.8294310648952874\n"
     ]
    }
   ],
   "source": [
    "param_dist_tree = {\n",
    "     'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': randint(1, 20),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20)\n",
    "}\n",
    "tree = DecisionTreeClassifier(random_state=RANDOM_STATE_ID)\n",
    "\n",
    "#Tuning using RandomizedSearchCV\n",
    "random_search_tree = RandomizedSearchCV(tree, param_distributions=param_dist_tree, \n",
    "                                   cv=3, scoring='accuracy',\n",
    "                                   n_iter=50, random_state=RANDOM_STATE_ID,\n",
    "                                   n_jobs=1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "random_search_tree.fit(X_train_processed, y_train)\n",
    "tuning_times[\"tree\"] = time.time() - start_time\n",
    "best_params= random_search_tree.best_params_\n",
    "best_score = random_search_tree.best_score_\n",
    "\n",
    "print(f\"Best Parameters (Random Search): {best_params}\")\n",
    "print(f\"Best Score (Random Search): {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "bb835659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8368181818181818\n",
      "{'tree': 4.079809904098511}\n"
     ]
    }
   ],
   "source": [
    "tuned_tree = random_search_tree.best_estimator_\n",
    "y_pred_tuned_tree = tuned_tree.predict(X_test_processed)\n",
    "\n",
    "accuracy_tree_tuned = accuracy_score(y_pred_tuned_tree, y_test)\n",
    "\n",
    "print(accuracy_tree_tuned)\n",
    "print(tuning_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "9590ffbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8213764442036524 0.8522599194327112\n"
     ]
    }
   ],
   "source": [
    "#Confidence Interval\n",
    "\n",
    "n = len(y_test)\n",
    "z = 1.96\n",
    "\n",
    "se_tree_tuned = np.sqrt((accuracy_tree_tuned*(1-accuracy_tree_tuned))/n) \n",
    "\n",
    "ci_upper = accuracy_tree_tuned + se_tree_tuned*z\n",
    "ci_lower = accuracy_tree_tuned - se_tree_tuned*z\n",
    "\n",
    "print(ci_lower, ci_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90119018",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style= \"font-family: 'Georgia', serif;\" >\n",
    "\n",
    "## <u> **Hyper-parameter tuning dicussion** </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021d4e46",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div style= \"font-family: 'Georgia', serif;\" >\n",
    "\n",
    "### **Results**\n",
    "\n",
    "| Model|Best Accuracy (Discrete Parametrical Space) | Best Accuracy (Continuous Parametrical Space) | Discrete Hyperparameter tuning time (s)|Continuous Hyperparameter tuning time (s)  |\n",
    "|-------|--------|---------|----------|-------|\n",
    "|Logistic Regression|0.8159 (GridSearch)|0.82 (Optuna)|3.35|1.75|\n",
    "|KNN|0.8191 (GridSearch)|0.8218 (Optuna)|10.68|3.22|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d402bb",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 1000px; font-family: 'Georgia', serif; font-size: 14px; line-height: 1.6;\">\n",
    "\n",
    "From the above table, we clearly see that hyper-parameter tuning using continuous parametrical space offers the greatest increase in accuracy for the best two base models (KNN and Logistic Regression). In the Continuous Parametrical Space, it is the Optuna method that performs better over the RandomizedSearch method, which is why we decided to only include the former in our results' table. With the results, we decide to use continuous parametrical space for future hyperparameter tuning.\n",
    "\n",
    "Furthermore, KNN displays the overall highest accuracy of our base models with 82.18 % . It also shows the greatest improvement in accuracy score compared to the Logistic Regression (2.79 % VS 0.22 % accuracy improvement). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093969aa",
   "metadata": {},
   "source": [
    "<div style= \"font-family: 'Georgia', serif;\" >\n",
    "\n",
    "## <u>**Advanced Models**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47b41ab",
   "metadata": {},
   "source": [
    "<div style= \"font-family: 'Georgia', serif;\" >\n",
    "\n",
    "### **SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "a51368e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score: 0.8563636363636363\n",
      "1.7182250022888184\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(random_state=RANDOM_STATE_ID)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "svm_model.fit(X_train_processed, y_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test_processed)\n",
    "\n",
    "accuracy_svm = accuracy_score(y_pred_svm, y_test)\n",
    "\n",
    "print(\"SVM Accuracy Score:\", accuracy_svm)\n",
    "print(training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "1d90c71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8417079541076244 0.8710193186196482\n"
     ]
    }
   ],
   "source": [
    "#Confidence Interval\n",
    "\n",
    "n = len(y_test)\n",
    "z = 1.96\n",
    "\n",
    "se_svm = np.sqrt((accuracy_svm*(1-accuracy_svm))/n) \n",
    "\n",
    "ci_upper = accuracy_svm + se_svm*z\n",
    "ci_lower = accuracy_svm- se_svm*z\n",
    "\n",
    "print(ci_lower, ci_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "81d8fa27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 13.775819585451138, 'kernel': 'rbf', 'gamma': 0.011632487356139776}\n",
      "Best score: 0.8609\n",
      "Time: 87.35s\n"
     ]
    }
   ],
   "source": [
    "#SVM tuning using Optuna\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def objective_svm(trial):\n",
    "    C = trial.suggest_float('C', 0.01, 100, log=True)               #Used AI for the suggest line.\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'rbf']) \n",
    "    gamma = trial.suggest_float('gamma', 0.0001, 1, log=True) \n",
    "    \n",
    "    model = SVC(C=C, kernel=kernel, gamma=gamma, random_state=RANDOM_STATE_ID)\n",
    "    model.fit(X_train_processed, y_train)  \n",
    "    \n",
    "    return accuracy_score(y_test, model.predict(X_test_processed))\n",
    "\n",
    "start = time.time()\n",
    "study_svm = optuna.create_study(direction='maximize')\n",
    "study_svm.optimize(objective_svm, n_trials=30)\n",
    "svm_time = time.time() - start\n",
    "\n",
    "print(\"Best params:\", study_svm.best_params)\n",
    "print(\"Best score:\", f\"{study_svm.best_value:.4f}\")\n",
    "print(\"Time:\", f\"{svm_time:.2f}s\")\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "897eb3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8464489424862109 0.875369239331971\n"
     ]
    }
   ],
   "source": [
    "#Confidence Interval\n",
    "\n",
    "n = len(y_test)\n",
    "z = 1.96\n",
    "\n",
    "se_svm = np.sqrt((study_svm.best_value*(1-study_svm.best_value))/n) \n",
    "\n",
    "ci_upper = study_svm.best_value + se_svm*z\n",
    "ci_lower = study_svm.best_value- se_svm*z\n",
    "\n",
    "print(ci_lower, ci_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3e245b",
   "metadata": {},
   "source": [
    "<div style= \"font-family: 'Georgia', serif;\" >\n",
    "\n",
    "### **Random Forest Classifier**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "7e65f18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy score: 0.8522727272727273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state=RANDOM_STATE_ID)\n",
    "start_time = time.time()\n",
    "\n",
    "rf.fit(X_train_processed, y_train)\n",
    "training_times[\"rf\"] = time.time() - start_time\n",
    "\n",
    "y_pred_rf= rf.predict(X_test_processed)\n",
    "\n",
    "accuracy_rf = accuracy_score(y_pred_rf, y_test)\n",
    "\n",
    "print(\"Random Forest Accuracy score:\", accuracy_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "42daf178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8374453490231826 0.867100105522272\n"
     ]
    }
   ],
   "source": [
    "#Confidence Interval\n",
    "\n",
    "n = len(y_test)\n",
    "z = 1.96\n",
    "\n",
    "se_rf = np.sqrt((accuracy_rf*(1-accuracy_rf))/n) \n",
    "\n",
    "ci_upper = accuracy_rf + se_rf*z\n",
    "ci_lower = accuracy_rf- se_rf*z\n",
    "\n",
    "print(ci_lower, ci_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7679d88a",
   "metadata": {},
   "source": [
    "<div style= \"font-family: 'Georgia', serif;\" >\n",
    "\n",
    "### **XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "0a236e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost accuracy score: 0.8563636363636363\n",
      "0.363527774810791\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,        \n",
    "    max_depth=6,             \n",
    "    learning_rate=0.1,      \n",
    "    objective='binary:logistic', \n",
    "    random_state=RANDOM_STATE_ID\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "xgb_model.fit(X_train_processed, y_train)\n",
    "training_times = time.time() - start_time\n",
    "y_pred_xgb = xgb_model.predict(X_test_processed)\n",
    "\n",
    "accuracy_xgb = accuracy_score(y_pred_xgb, y_test)\n",
    "\n",
    "print(\"XGBoost accuracy score:\", accuracy_xgb)\n",
    "print(training_times)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "df2c79f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8417079541076244 0.8710193186196482\n"
     ]
    }
   ],
   "source": [
    "#Confidence Interval\n",
    "\n",
    "n = len(y_test)\n",
    "z = 1.96\n",
    "\n",
    "se_xgb = np.sqrt((accuracy_xgb*(1-accuracy_xgb))/n) \n",
    "\n",
    "ci_upper = accuracy_xgb + se_xgb*z\n",
    "ci_lower = accuracy_xgb - se_xgb*z\n",
    "\n",
    "print(ci_lower, ci_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "5a535964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'max_depth': 4, 'learning_rate': 0.26299808743114744, 'n_estimators': 200}\n",
      "Best score: 0.8623\n",
      "Time: 11.73s\n"
     ]
    }
   ],
   "source": [
    "#Tuning XGBoost with Optuna\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "    \n",
    "    model = xgb.XGBClassifier(\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=n_estimators,\n",
    "        random_state=RANDOM_STATE_ID\n",
    "    )\n",
    "    model.fit(X_train_processed, y_train)\n",
    "    \n",
    "    return accuracy_score(y_test, model.predict(X_test_processed))\n",
    "\n",
    "start = time.time()\n",
    "study_xgb = optuna.create_study(direction='maximize')\n",
    "study_xgb.optimize(objective_xgb, n_trials=30)\n",
    "xgb_time = time.time() - start\n",
    "\n",
    "print(\"Best params:\", study_xgb.best_params)\n",
    "print(\"Best score:\", f\"{study_xgb.best_value:.4f}\")\n",
    "print(\"Time:\", f\"{xgb_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "34cbb8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8478722452340887 0.8766732093113659\n"
     ]
    }
   ],
   "source": [
    "#Confidence Interval\n",
    "\n",
    "n = len(y_test)\n",
    "z = 1.96\n",
    "\n",
    "se_xgb_tuned = np.sqrt((study_xgb.best_value*(1-study_xgb.best_value))/n) \n",
    "\n",
    "ci_upper = study_xgb.best_value + se_xgb_tuned*z\n",
    "ci_lower = study_xgb.best_value - se_xgb_tuned*z\n",
    "\n",
    "print(ci_lower, ci_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c95066",
   "metadata": {},
   "source": [
    "<div style= \"font-family: 'Georgia', serif;\" >\n",
    "\n",
    "## <u>**Results**</u>\n",
    "\n",
    "| Model | Accuracy - no hyperparameter tuning |95 % Confidence Interval| Accuracy - tuned hyperparameters |95% Confidence Interval| Accuracy Improvement (%)| Training time (s) | Hyperparameter tuning time (s)|\n",
    "|-------|--------|---------|----------|-----------|------------|--------|-------|\n",
    "|Logistic Regression|0.8182|[0.802, 0.834]|0.82|[0.799, 0.8317]|0.22||1.75|\n",
    "|KNN|0.7995|[0.783, 0.816]|0.8218|[0.800, 0.8326]|2.79|0.0039|3.22|\n",
    "|Decision Tree|0.785|[0.7604, 0.7951]|0.8368|[0.8214, 0.8523]|6.6|0.0802|4.9904|\n",
    "|SVM|0.8564|[0.8417, 0.871]|0.86|[0.8455, 0.8745]|0.42|1.48|103.9|\n",
    "|Random Forest|0.8522|[0.837, 0.867]|/|/|/|1.02|/|\n",
    "|XGboost|0.8564|[0.8417, 0.871]|0.8645|[0.85, 0.879]|0.946|0.174|9.57|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f272ab3",
   "metadata": {},
   "source": [
    "<div style=\"max-width: 1000px; font-family: 'Georgia', serif; font-size: 14px; line-height: 1.6;\">\n",
    "\n",
    "\n",
    "## <u>**Comments**</u>\n",
    "\n",
    "### <u>**Base Models**</u>\n",
    "From the above table, we note that out off the base models, the logistic regression performed the best with an accuracy score of 81.8 %. Then came the KNN method and finally the Decision Tree with accuracies of 79.95% and 78.5% respectively. \n",
    "\n",
    "\n",
    "\n",
    "#### **After hyperparameter tuning** \n",
    "After hyperparameter tuning, the KNN method achieved both the highest accuracy with 83.68% \n",
    "and the greatest improvement in accuracy (6.6%) compared to the Logistic Regression. Hyper-parameter tuning was slightly slower for KNN (3.22 seconds vs 1.75 seconds) The Logistic Regression model only experienced a slight increase in accuracy after tuning the hyperparameters (0.22%). \n",
    "\n",
    "Out of curiosity, we also decided to tune the Decision Tree model even though it performed worst out of the base models. Surprisingly, it ultimately displayed the highest improvement in accuracy  (6.6%) and best overall accuracy for the base models (83.68%).   \n",
    "\n",
    "\n",
    "### <u>**Advanced Models**</u>\n",
    "\n",
    "All three of the  (untuned) advanced models performed better than the base models before and after hyperparameter tuning. They all present similar accuracy scores with SVM and XGboost achieving a slightly higher (and identical) accuracy of 85.64 %. \n",
    "\n",
    "We checked the confusion matrix for XGboost and SVM and noticed that they had different numbers of correct predictions for class 0 and class 1 (has the client subscribed a term deposit?) but overall achieved the same number of total correct predictions, which explains why they achieved identical accuracy scores.\n",
    "\n",
    "#### **After hyperparameter tuning**\n",
    "\n",
    "We decided to tune the SVM and XGBoost models as they presented the highest accuracy of the advanced models. XGBoost achieved the higher accuracy score increase of the 2 (0.946%), with a final accuracy of 86.45%. It also had the faster hyperparameter tuning time with 9.57 seconds compared to 103.9 seconds for SVM (only improving accuracy by 0.42%)\n",
    "\n",
    "### <u>**Conclusion**</u>\n",
    "\n",
    "We conclude that we should use the more advanced prediction models for this problem as they all presented higher accuracy scores than the base models. We would favour the XGboost model as it presented the highest overall accuracy of all models (tuned and untuned) and as tuning was faster for that specifc model. \n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8f113c",
   "metadata": {},
   "source": [
    "<div style= \"font-family: 'Georgia', serif;\" >\n",
    "\n",
    "\n",
    "## <u>**Final Model**</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "3e5d377e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'job', 'marital', 'education', 'default', 'balance', 'housing',\n",
      "       'loan', 'contact', 'day', 'month', 'duration', 'campaign', 'pdays',\n",
      "       'previous', 'poutcome'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Load test dataset\n",
    "\n",
    "test_set = pd.read_pickle(COMPETITION_DATA_PATH)\n",
    "\n",
    "print(test_set.columns)\n",
    "\n",
    "# make predictions about this using the best method\n",
    "\n",
    "# save final model (in notes)\n",
    "# save predictions in a file (pkl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
